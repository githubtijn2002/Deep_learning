{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is: torch.Size([16, 3, 32, 32])\n",
      "unfolded size is: torch.Size([16, 27, 1024])\n",
      "unfolded size is: torch.Size([16, 1024, 27])\n",
      "weight size is: torch.Size([1, 3, 3, 3])\n",
      "weight size is: torch.Size([1, 27])\n",
      "output size is: torch.Size([16, 1024, 1])\n",
      "output size is: torch.Size([16, 1, 32, 32])\n",
      "input size is: torch.Size([16, 3, 32, 32])\n",
      "unfolded size is: torch.Size([16, 27, 1024])\n",
      "unfolded size is: torch.Size([16, 1024, 27])\n",
      "weight size is: torch.Size([1, 3, 3, 3])\n",
      "weight size is: torch.Size([1, 27])\n",
      "output size is: torch.Size([16, 1024, 1])\n",
      "output size is: torch.Size([16, 1, 32, 32])\n",
      "Output (built-in) size: torch.Size([16, 1, 32, 32])\n",
      "Output (custom) size: torch.Size([16, 1, 32, 32])\n",
      "False\n",
      "tensor(20.5723, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1,padding=1):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, *kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "        print(f'input size is: {input_batch.size()}')\n",
    "        unfolded = F.unfold(input_batch, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        # flatten the patches (with all channels) into vectors, arranged as the rows of a matrix X\n",
    "        print(f'unfolded size is: {unfolded.size()}')\n",
    "        unfolded = unfolded.transpose(1, 2).contiguous().view(b, -1, c * self.kernel_size[0] * self.kernel_size[1])\n",
    "        print(f'unfolded size is: {unfolded.size()}')\n",
    "        # flatten the weight tensor into a matrix W\n",
    "        print(f'weight size is: {self.weight.size()}')\n",
    "        weight = self.weight.view(self.out_channels, -1)\n",
    "        print(f'weight size is: {weight.size()}')\n",
    "        # compute the matrix multiplication XW^T\n",
    "        output = torch.bmm(unfolded, weight.t().unsqueeze(0).expand(b, -1, -1))\n",
    "        print(f'output size is: {output.size()}')\n",
    "        # reshape the output to its final shape\n",
    "        output = output.view(b, self.out_channels, h, w)\n",
    "        # add the bias\n",
    "        output += self.bias.view(1, -1, 1, 1).expand_as(output)\n",
    "        print(f'output size is: {output.size()}')\n",
    "        return output\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "conv = Conv2D(3, 1)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)\n",
    "\n",
    "# Built-in Conv2D implementation\n",
    "conv_builtin = nn.Conv2d(3, 1, kernel_size=3, stride=1, padding=1)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_builtin = conv_builtin(input_batch)\n",
    "\n",
    "# Custom Conv2D usage\n",
    "conv_custom = Conv2D(3, 1)\n",
    "output_custom = conv_custom(input_batch)\n",
    "\n",
    "print(f'Output (built-in) size: {output_builtin.size()}')\n",
    "print(f'Output (custom) size: {output_custom.size()}')\n",
    "# check if the output of the custom implementation matches the output of the built-in implementation\n",
    "print(torch.allclose(output_builtin, output_custom))\n",
    "# if false, print the maximum absolute difference between the two outputs\n",
    "print(torch.max(torch.abs(output_builtin - output_custom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size is: torch.Size([16, 1, 32, 32])\n",
      "grad_output_unfolded size is: torch.Size([16, 1, 1024])\n",
      "grad_output_unfolded size is: torch.Size([16, 1024, 1])\n",
      "grad_output_unfolded size is: torch.Size([16384, 1])\n",
      "\n",
      "grad_input_unfolded size is: torch.Size([16384, 27])\n",
      "grad_input_unfolded size is: torch.Size([16, 27, 1024])\n",
      "grad_input size is: torch.Size([16, 3, 32, 32])\n",
      "False\n",
      "tensor(4.7684e-06, grad_fn=<MaxBackward1>)\n",
      "True\n",
      "True\n",
      "True\n",
      "tensor(0.)\n",
      "tensor(0.0004)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Conv2DFunc(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, bias=None, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        Forward pass for 2D convolution using unfold.\n",
    "        \"\"\"\n",
    "        # Save inputs for backward\n",
    "        ctx.save_for_backward(input_batch, kernel, bias)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        \n",
    "        # Pad the input\n",
    "        b, c, h, w = input_batch.size()\n",
    "        input_padded = F.pad(input_batch, (padding, padding, padding, padding))\n",
    "        \n",
    "        # Extract patches from the input tensor using unfold\n",
    "        unfolded_input = F.unfold(input_padded, kernel_size=kernel.size(2), stride=stride)\n",
    "        \n",
    "        # Save unfolded input for backward pass\n",
    "        ctx.unfolded_input = unfolded_input\n",
    "        \n",
    "        # Reshape unfolded input: (b, c * kh * kw, h_out * w_out)\n",
    "        unfolded_input = unfolded_input.view(b, c * kernel.size(2) * kernel.size(3), -1)\n",
    "        \n",
    "        # Reshape kernel to (out_channels, in_channels * kh * kw)\n",
    "        unfolded_kernel = kernel.view(kernel.size(0), -1)\n",
    "        \n",
    "        # Perform the matrix multiplication: (b, out_channels, h_out * w_out)\n",
    "        output = unfolded_kernel.matmul(unfolded_input)\n",
    "        \n",
    "        # Reshape output to (b, out_channels, h_out, w_out)\n",
    "        h_out = (h + 2 * padding - kernel.size(2)) // stride + 1\n",
    "        w_out = (w + 2 * padding - kernel.size(3)) // stride + 1\n",
    "        output = output.view(b, kernel.size(0), h_out, w_out)\n",
    "        print(f'output size is: {output.size()}')\n",
    "        \n",
    "        # Add bias if applicable\n",
    "        if bias is not None:\n",
    "            output += bias.view(1, -1, 1, 1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_batch, kernel, bias = ctx.saved_tensors\n",
    "        stride, padding = ctx.stride, ctx.padding\n",
    "        unfolded_input = ctx.unfolded_input\n",
    "        \n",
    "        b, c, h, w = input_batch.size()\n",
    "        out_channels, in_channels, kh, kw = kernel.size()\n",
    "        \n",
    "        # Compute gradients for input and kernel\n",
    "        grad_input = None\n",
    "        grad_kernel = None\n",
    "        grad_bias = None\n",
    "        \n",
    "        # 1. Gradient with respect to input\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_output_unfolded = grad_output.view(b, out_channels, -1)\n",
    "            print(f'grad_output_unfolded size is: {grad_output_unfolded.size()}')\n",
    "            grad_output_unfolded = grad_output_unfolded.permute(0, 2, 1).contiguous()\n",
    "            print(f'grad_output_unfolded size is: {grad_output_unfolded.size()}')\n",
    "            grad_output_unfolded = grad_output_unfolded.view(-1, out_channels)\n",
    "            print(f'grad_output_unfolded size is: {grad_output_unfolded.size()}\\n')\n",
    "            \n",
    "            kernel_reshaped = kernel.view(out_channels, -1)\n",
    "            grad_input_unfolded = grad_output_unfolded.matmul(kernel_reshaped)\n",
    "            print(f'grad_input_unfolded size is: {grad_input_unfolded.size()}')\n",
    "            grad_input_unfolded = grad_input_unfolded.view(b, -1, grad_output_unfolded.size(0) // b)\n",
    "            print(f'grad_input_unfolded size is: {grad_input_unfolded.size()}')\n",
    "            \n",
    "            grad_input = F.fold(grad_input_unfolded, (h + 2 * padding - kh + 1, w + 2 * padding - kw + 1), (kh, kw), stride=stride, padding=padding)\n",
    "            print(f'grad_input size is: {grad_input.size()}')\n",
    "        \n",
    "        # 2. Gradient with respect to kernel\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_output_unfolded = grad_output.view(b, out_channels, -1)\n",
    "            grad_output_unfolded = grad_output_unfolded.permute(0, 2, 1).contiguous()\n",
    "            grad_output_unfolded = grad_output_unfolded.view(-1, out_channels)\n",
    "            \n",
    "            unfolded_input = unfolded_input.permute(0, 2, 1).contiguous()\n",
    "            unfolded_input = unfolded_input.view(-1, unfolded_input.size(2))\n",
    "            \n",
    "            grad_kernel = grad_output_unfolded.t().matmul(unfolded_input)\n",
    "            grad_kernel = grad_kernel.view(out_channels, in_channels, kh, kw)\n",
    "        \n",
    "        # 3. Gradient with respect to bias\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(dim=(0, 2, 3))\n",
    "        \n",
    "        return grad_input, grad_kernel, grad_bias, None, None\n",
    "\n",
    "input_batch = torch.randn(16, 3, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(1, 3, 3, 3, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Apply the custom convolution function\n",
    "output = Conv2DFunc.apply(input_batch, kernel, bias, 1, 1)\n",
    "output.backward(torch.ones_like(output))\n",
    "\n",
    "# Compare with built-in convolution\n",
    "conv_builtin = nn.Conv2d(3, 1, kernel_size=3, stride=1, padding=1)\n",
    "conv_builtin.weight.data = kernel\n",
    "conv_builtin.bias.data = bias\n",
    "output_builtin = conv_builtin(input_batch)\n",
    "output_builtin.backward(torch.ones_like(output_builtin))\n",
    "\n",
    "# Check if the output of the custom implementation matches the output of the built-in implementation\n",
    "print(torch.allclose(output, output_builtin))\n",
    "# if false, print the maximum absolute difference between the two outputs\n",
    "print(torch.max(torch.abs(output - output_builtin)))\n",
    "\n",
    "# Check if the gradients of the custom implementation match the gradients of the built-in implementation\n",
    "print(torch.allclose(input_batch.grad, input_batch.grad))\n",
    "print(torch.allclose(kernel.grad, conv_builtin.weight.grad))\n",
    "print(torch.allclose(bias.grad, conv_builtin.bias.grad))\n",
    "# if false, print the maximum absolute difference between the gradients\n",
    "print(torch.max(torch.abs(input_batch.grad - input_batch.grad)))\n",
    "print(torch.max(torch.abs(kernel.grad - conv_builtin.weight.grad)))\n",
    "print(torch.max(torch.abs(bias.grad - conv_builtin.bias.grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[196], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 2\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1060 3GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "\n",
    "# Check which GPU PyTorch is using\n",
    "print(torch.cuda.current_device())  # Should print the device index\n",
    "print(torch.cuda.get_device_name(0))  # Should print the name of the GPU, e.g., 'GeForce GTX 1060'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from    torch.utils.data import DataLoader, random_split\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataset = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs = 5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        runnin_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runnin_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {runnin_loss / 100}')\n",
    "                runnin_loss = 0.0\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        print(f'Validation loss: {val_loss / len(val_loader)}')\n",
    "        print(f'Validation accuracy: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
